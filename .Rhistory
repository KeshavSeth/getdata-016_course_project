# get the matrix from the object
mat <- x$get()
# compute the inverse
inv <- solve(mat)
# set the inverse
x$setInverse(inv)
# return the inverse
inv
}
amatrix <- makeCacheMatrix(matrix(1:9, nrow= 3, ncol = 3))
amatrix$get()
cacheSolve(amatrix)
amatrix <- makeCacheMatrix(matrix(1:4, nrow= 2, ncol = 2))
cacheSolve(amatrix)
amatrix <- makeCacheMatrix(matrix(1:16, nrow= 4, ncol = 4))
cacheSolve(amatrix)
a <- matrix(1:16, nrow= 4, ncol = 4)
a
solve(a)
a <- matrix(1:25, nrow= 5, ncol = 5)
solve(a)
amatrix$set(matrix(c(0,5,99,66), nrow=2, ncol=2))
amatrix$get()
cacheSolve(amatrix)
amatrix$getInverse()
ws <- c(ls())
rm(ws, list = ws)
set.seed(20)
x <- rnorm(100)
e <- rnorm(100, 0, 2)
y <- 0.5 + 2 * x + e
summary(y)
plot(x, y)
Rprof()
summaryRprof()
set.seed(1)
rpois(5, 2)
set.seed(1)
rpois(5, 2)
set.seed(1)
rpois(5, 2)
library("swirl")
swirl
swirl()
library("swirl")
swirl()
install.packages("dplyr")
install.packages(c("jsonlite", "RCurl", "RSQLite"))
install.packages("Rcpp")
install.packages("dplyr")
install.packages("tidyr")
install.packages("lubridate")
library("swirl")
swirl()
mydf <- read.csv(path2csv, stringAsFactors = FALSE)
?read.csv
mydf <- read.csv(path2csv, stringAsFactors = FALSE, ...)
mydf <- read.csv(path2csv, stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
packageVersion("Rcpp")
cran <- tbl_df(mydf)
rm("mydf")
?tbl_df
cran
?select
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran, !is.na(r_version))
cran2 <- select(cran, size:ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id))
arrange(cran2, package, ip_id)
arrange(cran2, country, desc(r_version, ip_id))
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran, ip_id, package, size)
cran3
mutate(cran3, size_mb = size / 2 ^ 20)
mutate(cran3, size_gb = size_mb / 2 ^ 20)
mutate(cran3, size_mb = size / 2 ^ 20, size_gb = size_mb / 2 ^ 20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size-1000)
mutate(cran3, correct_size = size+1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
tbl_df(mydf)
cran <- tbl_df(mydf)
rm("mydf")
cran
group_by(cran, package)
?group_by
by_package <- group_by(cran, package)
by_package
summarize(by_package, mean(size))
reset()
?n
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
filter(pack_sum, count > 679)
top_counts <- filter(pack_sum, count > 679)
top_counts
head(top_counts, 20)
arrange(top_counts, desc(count))
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique>465)
top_unique
arrange(top_unique, desc(unique))
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
library(tidyr)
students
?gather
gather(students, sex, count, -grade)
students2
res <- gather(students2, sex_class, count, -grade)
res
?separate
separate(data = res,col = sex_class, into = c("sex", "class"))
submit()
student3
students3
?gather
submit()
submit()
?spread
submit()
submit()
submit()
submit()
submit()
extract_numeric("class5")
extract_numeric("cl3ass5")
extract_numeric("cl3as0s5")
submit()
students4
submit()
?unique
submit()
submit()
submit()
passed
failed
passed <- mutate(passed, status = "passed")
failed <- mutate(failed, status = "failed")
?rbind_list
rbind_list(passed, failed)
sat
?select
submit()
submit()
?group_by
submit()
submit()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package = lubridate)
a <- c(ls())
rm(a, list = a)
this_day <- today()
swirl()
this_day
rm(a)
year(this_day)
?rm
rm(list = a)
wday(this_day)
wday(this_day, label = TRUE)
now()
this_moment <- now()
this_moment
hour(this_moment)
ymd("1989-05-17")
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
ymd("19489 May 17")
mdy("March 12, 1975")
dmy(25081985)
ymd("192012")
ymd("1920-1-2")
dt1
swirl()
dt1
ymd_hms(dt1)
hms("03:22:14")
dt2
ymd(dt2)
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
this_moment <- update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
?now
nyc <- now("America/New_York")
nyc
nyc + days(2)
depart <- nyc + days(2)
depart
depart <- update(depart, hours = 17, minutes = 34)
depart
arrive <- update(depart, hours = 15, minutes = 50)
arrive <- depart  + hours(15) + minutes(50)
?with_tz
arrive <- with_tz(arrive, "Asia/Hong_Kong")
arrive
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time
?new_interval
how_long <- new_interval(last_time, arrive)
as.period(how_long)
stopwatch()
clear
exit
version()
sessionInfo()
fuck()
fuck.you
library(installR)
install.packages(installR)
install.packages("installR")
install.packages("installr")
install.packages("RMySQL")
install.packages("RMySQL")
install.packages("RMySQL")
install.packages("RMySQL")
library(RMySQL)
ucscDb <- dbConnect(MySQL(), user = "genome", )
ucscDb <- dbConnect(MySQL(), user = "genome", host = "genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb, "show databeses;"); dbDisconnect(ucscDb);
result <- dbGetQuery(ucscDb, "show databeses;")
result <- dbGetQuery(ucscDb, "show databases;")
dbDisconnect(ucscDb)
ucscDb
result
hg19 < dbConnect(MySQL(), user= "genome", db = "hg19", host = "genome-mysql.cse.ucsc.edu")
hg19 <- dbConnect(MySQL(), user= "genome", db = "hg19", host = "genome-mysql.cse.ucsc.edu")
allTables <- dbListTable(hg19)
allTables <- dbListTables(hg19)
allTables[1:5]
dbListFields(hg19, "affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
affyData <- dbReadTable(hg19, "affyU133Plus2")
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
upgrade
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
library(httr)
oauth_endpoints("github")
myapp <- oauth_app('github', key = "2e1ff61e7eb7c7cdc70b", secret = "3cc5ca91c80e7c1c96d0b4c9c25454f890c3c804")
github_token <- oauth2.0_token(oauth_enpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
myapp <- oauth_app('github', key = "847bd73fdf8bf8be8bd5", secret = "0f51f89ea2203ee0d4c48eb0a0a539cf06a526eb")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
myapp <- oauth_app('github', key = "afe7b8e5b4057d6d3291", secret = "3becde6357a87b30a581d7c00b8d7c2f031cda16")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
kawaii
lkjklkddkddkllkdlkklklkjdlkdjlfsdjkfdjlddjldjkdjlddlkjkfdjljdljkjldssjkjkdkdfakfjdaf;daslfksdjf;alsdkfjdsaflkdfjaweoijeoiejfkldnclkvnlkghoifkfadjfdl;alc.zvndakfjoqirjkdjf.,ncvlkjdfiejkfadvc,zvnkadfje;oigkljafdkhgkabnab;dklfjdafidjckxvmdnladifeiownksdjflakdnvlaidfhoeaifheiakfjdsklfandsivdnflkjwheiohfdklsjdio;vj qlkwihfdklfnkej;lkjfdopvnkdlnfej;qfiodhvlafklqjeoiufdkjfkljeiwnkdnfkldafjcivohkdjfiehfkdnvlkadjf;dkjvidhgdklajfkd;jfadlfkdlafkjd;fldjvoibhflkajfd;fkdjvihda;flkvjdisahfldkfjdsoihfvdoqpdnvkadhklfdqiofulkdfjkeqnflkdfjoiqjkljfklvndakljfeoiqjflkd;kajfkljdfiodjfflka;jfioajfldfkjdfiojqlfnzlvnc.a/fkldfa;dkfjdklfjaoidhvcv,mnadhgeyqhdkashfljqif;dajkjfivhdkaljJ:'ajkjf;da;kfa;kfldkfjhdjfhdjafhldkfa'fadfkhihfluhqjkef;dfhkahaefp
fuck you
penis
library(datasets)
library(lattice)
airquality <- transform(airquality, Month = factor(Month))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(6, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(4, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(4, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(3, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(2, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(1, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(4, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(10, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(6, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(100, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(1000, 1))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5, 1))
airquality
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
xyplot(weight ~ Time)
xyplot(weight ~ Time | Diet)
xyplot(weight ~ Time | Diet, BodyWeight)
xyplot(weight ~ Time | Diet, BodyWeight)
?xyplot
xyplot(weight ~ Time , BodyWeight)
xyplot(weight ~ Time | Diet, BodyWeight)
?points
?panel.abline
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
p
?par
?splom
?print.trellis
?trellis.par.set
airquality = transform(airquality, Month = factor(Month))
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
library(ggplot2)
qplot(Wind, Ozone, data = airquality, facets = . ~ Month)
qplot(Wind, Ozone, data = airquality, geom = "smooth")
qplot(Wind, Ozone, data = airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
?geom
info(geom)
g <- ggplot(movies, aes(votes, rating))
g <- ggplot(movies, aes(votes, rating))
print(g)
##############################################
# Set working directory to the directory you #
# downloaded the data in.                    #
##############################################
#> setwd("Your directory here");
#> list.files();
#[1] "run_analysis.R"  "UCI HAR Dataset"
##############################################
# 1. Merging the training and test data sets #
##############################################
# Reading the training set #
train_x <- read.table("./UCI HAR Dataset/train/X_train.txt")
train_y <- read.table("./UCI HAR Dataset/train/y_train.txt")
train_subject <- read.table("./UCI HAR Dataset/train/subject_train.txt")
# Reading the test set #
test_x <- read.table("./UCI HAR Dataset/test/X_test.txt")
test_y <- read.table("./UCI HAR Dataset/test/y_test.txt")
test_subject <- read.table("./UCI HAR Dataset/test/subject_test.txt")
# Merging train_x, train_y, data_subject
data_x <- rbind(train_x, test_x)
data_y <- rbind(train_y, test_y)
data_subject <- rbind(train_subject, test_subject)
# removing unnecessary data
l <- c("test_x", "test_y", "test_subject", "train_x", "train_y",
"train_subject")
rm(list = l)
################################################
# 2. Extracting the features from features.txt #
################################################
# Loading features.txt to features
features <- read.table("./UCI HAR Dataset//features.txt")
# Give the features names to data_x
colnames(data_x) <- features$V2
# Create a logical vector containing whether the feature has
# either mean() or std() pattern in them
# grepl uses the regex expression and the features as arguments and
# returns a logical vector
features_logical <- grepl("-(mean|std)\\(\\)", features$V2)
#> backup_x <- data_x
# subset data_x so that it now only contains the values of mean() and std()
data_x <- data_x[ ,features_logical]
# removing unnecessary data
l <- c("features", "features_logical")
rm(list = l)
#######################################################
# 3. Assinging activity name to Activities in dataset #
#######################################################
# reading activities from activity_lables.txt
activity_lables <- read.table("./UCI HAR Dataset/activity_labels.txt")
# labeling the columns of the activity_lables data frame
colnames(activity_lables) <- c("ID", "Activity")
# Giving the descriptive activity label
data_y[, 1] <- activity_lables[data_y[, 1], 2]
# Labeling the column
colnames(data_y) <- "Activity"
# removing unnecessary data
rm(activity_lables)
#######################################################
# 4. Labeling dataset with descriptive variable names #
#######################################################
# data_x and data_y is already lablled
# labeling the data_subject
colnames(data_subject) <- "Subject"
# Now we can combine the data_x, data_y, data_subject
data <- cbind(data_subject, data_y, data_x)
# removing unnecessary data
l <- c("data_x", "data_y", "data_subject")
rm(l, list = l)
###################################################
# 5. Creating a second independent tidy data set. #
#    Average each variable for each activity      #
#    each subject.                                #
###################################################
# Load the plyr library
library(plyr)
# ddply splits the data frame,
# apply function, and returns result in a data frame
data_averaged <- ddply(data, .(Subject, Activity),
function(data) colMeans(data[,3:68]))
## NOTE :
## Now each column in above dataset is a variable and each row comprises of
## an observation. Hence it is Tidy dataset.
# write the output file
write.table(data_averaged, "data_average.txt")
# Read the data_average.txt using the command
#> data <- read.table("./data_average.txt")
#> View(data)
setwd("/home/potato_head/DS/3. Getting and Cleaning Data /Course Project/")
##############################################
# Set working directory to the directory you #
# downloaded the data in.                    #
##############################################
#> setwd("Your directory here");
#> list.files();
#[1] "run_analysis.R"  "UCI HAR Dataset"
##############################################
# 1. Merging the training and test data sets #
##############################################
# Reading the training set #
train_x <- read.table("./UCI HAR Dataset/train/X_train.txt")
train_y <- read.table("./UCI HAR Dataset/train/y_train.txt")
train_subject <- read.table("./UCI HAR Dataset/train/subject_train.txt")
# Reading the test set #
test_x <- read.table("./UCI HAR Dataset/test/X_test.txt")
test_y <- read.table("./UCI HAR Dataset/test/y_test.txt")
test_subject <- read.table("./UCI HAR Dataset/test/subject_test.txt")
# Merging train_x, train_y, data_subject
data_x <- rbind(train_x, test_x)
data_y <- rbind(train_y, test_y)
data_subject <- rbind(train_subject, test_subject)
# removing unnecessary data
l <- c("test_x", "test_y", "test_subject", "train_x", "train_y",
"train_subject")
rm(list = l)
################################################
# 2. Extracting the features from features.txt #
################################################
# Loading features.txt to features
features <- read.table("./UCI HAR Dataset//features.txt")
# Give the features names to data_x
colnames(data_x) <- features$V2
# Create a logical vector containing whether the feature has
# either mean() or std() pattern in them
# grepl uses the regex expression and the features as arguments and
# returns a logical vector
features_logical <- grepl("-(mean|std)\\(\\)", features$V2)
#> backup_x <- data_x
# subset data_x so that it now only contains the values of mean() and std()
data_x <- data_x[ ,features_logical]
# removing unnecessary data
l <- c("features", "features_logical")
rm(list = l)
#######################################################
# 3. Assinging activity name to Activities in dataset #
#######################################################
# reading activities from activity_lables.txt
activity_lables <- read.table("./UCI HAR Dataset/activity_labels.txt")
# labeling the columns of the activity_lables data frame
colnames(activity_lables) <- c("ID", "Activity")
# Giving the descriptive activity label
data_y[, 1] <- activity_lables[data_y[, 1], 2]
# Labeling the column
colnames(data_y) <- "Activity"
# removing unnecessary data
rm(activity_lables)
#######################################################
# 4. Labeling dataset with descriptive variable names #
#######################################################
# data_x and data_y is already lablled
# labeling the data_subject
colnames(data_subject) <- "Subject"
# Now we can combine the data_x, data_y, data_subject
data <- cbind(data_subject, data_y, data_x)
# removing unnecessary data
l <- c("data_x", "data_y", "data_subject")
rm(l, list = l)
###################################################
# 5. Creating a second independent tidy data set. #
#    Average each variable for each activity      #
#    each subject.                                #
###################################################
# Load the plyr library
library(plyr)
# ddply splits the data frame,
# apply function, and returns result in a data frame
data_averaged <- ddply(data, .(Subject, Activity),
function(data) colMeans(data[,3:68]))
## NOTE :
## Now each column in above dataset is a variable and each row comprises of
## an observation. Hence it is Tidy dataset.
# write the output file
write.table(data_averaged, "data_average.txt")
# Read the data_average.txt using the command
#> data <- read.table("./data_average.txt")
#> View(data)
